"""
Summarize service ƒë·ªÉ t√≥m t·∫Øt chunks v·ªõi Gemini API
"""

import os
import sys
import json
import time
import asyncio
from typing import List, Dict, Any

# Add parent directory to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from langchain_google_genai import ChatGoogleGenerativeAI
from utils.quota_manager import QuotaManager

# Initialize Gemini model
def get_llm():
    """Get Gemini model instance"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY environment variable not set")
    
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        api_key=api_key
    )

# Initialize quota manager
quota_manager = QuotaManager()

# Import summarize prompt
from prompts.summarize_prompt import SUMMARIZE_PROMPT

async def summarize_chunks(chunks: List[Dict[str, Any]], max_chunks_per_batch: int = 10) -> Dict[str, Any]:
    """
    T√≥m t·∫Øt chunks theo ph∆∞∆°ng ph√°p m·ªõi:
    1. X·ª≠ l√Ω 5-10 chunk t·ªëi ƒëa ƒë·ªÉ t·∫°o b·∫£n t√≥m t·∫Øt nh·ªè
    2. L∆∞u l·∫°i b·∫£n t√≥m t·∫Øt nh·ªè
    3. L·∫∑p l·∫°i quy tr√¨nh cho ƒë·∫øn khi h·∫øt chunk
    4. G·ªôp c√°c b·∫£n t√≥m t·∫Øt nh·ªè th√†nh b·∫£n cu·ªëi c√πng
    
    Args:
        chunks: List chunks c·∫ßn t√≥m t·∫Øt
        max_chunks_per_batch: S·ªë chunks t·ªëi ƒëa m·ªói batch (default: 10)
    
    Returns:
        Dict ch·ª©a video_id v√† text t√≥m t·∫Øt cu·ªëi c√πng
    """
    if not chunks:
        return {"video_id": None, "text": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t√≥m t·∫Øt"}
    
    video_id = chunks[0].get('video_id', 'unknown')
    lesson_title = chunks[0].get('lesson_title', 'unknown')
    print(f"üìù Summarizing {len(chunks)} chunks for video_id: {video_id}, lesson_title: {lesson_title}")
    
    # Chia chunks th√†nh batches
    batches = []
    for i in range(0, len(chunks), max_chunks_per_batch):
        batch = chunks[i:i + max_chunks_per_batch]
        batches.append(batch)
    
    print(f"üì¶ Created {len(batches)} batches (max {max_chunks_per_batch} chunks per batch)")
    
    # L∆∞u tr·ªØ c√°c b·∫£n t√≥m t·∫Øt nh·ªè trong bi·∫øn
    small_summaries = []
    
    # B∆∞·ªõc 1-3: X·ª≠ l√Ω t·ª´ng batch v√† l∆∞u b·∫£n t√≥m t·∫Øt nh·ªè v√†o bi·∫øn
    for i, batch in enumerate(batches):
        print(f"\nüîÑ Processing batch {i+1}/{len(batches)}...")
        
        # G·ªôp text t·ª´ t·∫•t c·∫£ chunks trong batch
        batch_text = " ".join([chunk['text'] for chunk in batch])
        
        print(f"  Text length: {len(batch_text)} characters")
        print(f"  Chunks: {len(batch)} chunks")
        
        try:
            # T√≥m t·∫Øt batch n√†y (kh√¥ng c·∫ßn previous summary)
            small_summary = await summarize_batch_simple(batch_text)
            small_summaries.append(small_summary)
            print(f"  ‚úÖ Batch {i+1} summarized successfully")
            
        except Exception as e:
            print(f"  ‚ùå Error summarizing batch {i+1}: {e}")
            # Fallback: s·ª≠ d·ª•ng text g·ªëc n·∫øu API fail
            fallback_summary = f"[Batch {i+1} - API Error] {batch_text[:200]}..."
            small_summaries.append(fallback_summary)
    
    # B∆∞·ªõc 4: G·ªôp c√°c b·∫£n t√≥m t·∫Øt nh·ªè th√†nh b·∫£n cu·ªëi c√πng
    print(f"\nüîó Combining {len(small_summaries)} small summaries into final summary...")
    
    try:
        final_summary = await combine_small_summaries(small_summaries)
        print(f"‚úÖ Final summary created successfully")
        
    except Exception as e:
        print(f"‚ùå Error combining summaries: {e}")
        # Fallback: n·ªëi c√°c summary nh·ªè l·∫°i
        final_summary = "\n\n".join(small_summaries)
    
    result = {
        "video_id": video_id,
        "lesson_title": lesson_title,
        "text": final_summary
    }
    
    print(f"\n‚úÖ Summarization completed!")
    print(f"üìä Final summary length: {len(final_summary)} characters")
    print(f"üìä Small summaries count: {len(small_summaries)}")
    
    # Th√™m small_summaries v√†o result ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng
    result["small_summaries"] = small_summaries
    
    return result

async def summarize_batch_simple(text: str, max_retries: int = 3) -> str:
    """
    T√≥m t·∫Øt ƒë∆°n gi·∫£n m·ªôt batch text (kh√¥ng c·∫ßn previous summary)
    
    Args:
        text: Text c·∫ßn t√≥m t·∫Øt
        max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
    
    Returns:
        Text t√≥m t·∫Øt
    """
    for attempt in range(max_retries):
        try:
            # Get LLM instance
            llm = get_llm()
            
            # S·ª≠ d·ª•ng prompt ƒë∆°n gi·∫£n cho batch
            simple_prompt = f"""
H√£y t√≥m t·∫Øt n·ªôi dung sau m·ªôt c√°ch ng·∫Øn g·ªçn v√† s√∫c t√≠ch b·∫±ng ti·∫øng Vi·ªát:

{text}

T√≥m t·∫Øt:
"""
            
            # G·ªçi Gemini API
            result = llm.invoke(simple_prompt)
            quota_manager.record_request()
            
            # Extract text t·ª´ response
            if hasattr(result, 'content'):
                return result.content.strip()
            else:
                return str(result).strip()
                
        except Exception as e:
            error_msg = str(e).lower()
            
            # Check if it's a rate limit error
            if any(keyword in error_msg for keyword in ['quota', 'rate', 'limit', 'exceeded', '429']):
                if attempt < max_retries - 1:
                    delay = quota_manager.handle_quota_error(str(e))
                    print(f"    ‚ö†Ô∏è  Rate limit hit, waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
                    time.sleep(delay)
                    continue
                else:
                    print(f"    ‚ùå Max retries exceeded for batch summarization")
                    raise e
            else:
                print(f"    ‚ùå API error: {e}")
                raise e
    
    raise Exception("All retry attempts failed")

async def combine_small_summaries(small_summaries: List[str], max_retries: int = 3) -> str:
    """
    G·ªôp c√°c b·∫£n t√≥m t·∫Øt nh·ªè th√†nh b·∫£n cu·ªëi c√πng
    
    Args:
        small_summaries: List c√°c b·∫£n t√≥m t·∫Øt nh·ªè
        max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
    
    Returns:
        B·∫£n t√≥m t·∫Øt cu·ªëi c√πng
    """
    if not small_summaries:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ g·ªôp"
    
    if len(small_summaries) == 1:
        return small_summaries[0]
    
    # G·ªôp t·∫•t c·∫£ summaries th√†nh m·ªôt text
    combined_text = "\n\n".join([f"Ph·∫ßn {i+1}: {summary}" for i, summary in enumerate(small_summaries)])
    
    for attempt in range(max_retries):
        try:
            # Get LLM instance
            llm = get_llm()
            
            # Prompt ƒë·ªÉ g·ªôp summaries
            combine_prompt = f"""
H√£y g·ªôp v√† t√≥m t·∫Øt l·∫°i c√°c ph·∫ßn t√≥m t·∫Øt sau th√†nh m·ªôt b·∫£n t√≥m t·∫Øt cu·ªëi c√πng ho√†n ch·ªânh v√† m·∫°ch l·∫°c b·∫±ng ti·∫øng Vi·ªát:

{combined_text}

B·∫£n t√≥m t·∫Øt cu·ªëi c√πng:
"""
            
            # G·ªçi Gemini API
            result = llm.invoke(combine_prompt)
            quota_manager.record_request()
            
            # Extract text t·ª´ response
            if hasattr(result, 'content'):
                return result.content.strip()
            else:
                return str(result).strip()
                
        except Exception as e:
            error_msg = str(e).lower()
            
            # Check if it's a rate limit error
            if any(keyword in error_msg for keyword in ['quota', 'rate', 'limit', 'exceeded', '429']):
                if attempt < max_retries - 1:
                    delay = quota_manager.handle_quota_error(str(e))
                    print(f"    ‚ö†Ô∏è  Rate limit hit, waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
                    time.sleep(delay)
                    continue
                else:
                    print(f"    ‚ùå Max retries exceeded for combining summaries")
                    raise e
            else:
                print(f"    ‚ùå API error: {e}")
                raise e
    
    raise Exception("All retry attempts failed")

async def summarize_with_retry(text: str, previous_summary: str = "", max_retries: int = 3) -> str:
    """
    T√≥m t·∫Øt text v·ªõi retry logic cho rate limiting
    
    Args:
        text: Text c·∫ßn t√≥m t·∫Øt
        max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
    
    Returns:
        Text t√≥m t·∫Øt
    """
    for attempt in range(max_retries):
        try:
            # Get LLM instance
            llm = get_llm()
            
            # G·ªçi Gemini API
            result = llm.invoke(SUMMARIZE_PROMPT.format(text=text, previous_summary=previous_summary))
            quota_manager.record_request()
            
            # Extract text t·ª´ response
            if hasattr(result, 'content'):
                return result.content.strip()
            else:
                return str(result).strip()
                
        except Exception as e:
            error_msg = str(e).lower()
            
            # Check if it's a rate limit error
            if any(keyword in error_msg for keyword in ['quota', 'rate', 'limit', 'exceeded', '429']):
                if attempt < max_retries - 1:
                    delay = quota_manager.handle_quota_error(str(e))
                    print(f"    ‚ö†Ô∏è  Rate limit hit, waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
                    time.sleep(delay)
                    continue
                else:
                    print(f"    ‚ùå Max retries exceeded for summarization")
                    raise e
            else:
                print(f"    ‚ùå API error: {e}")
                raise e
    
    raise Exception("All retry attempts failed")

def summarize_from_file(input_file: str = "chunked_transcript.json", 
                       output_file: str = "summarized_report.json",
                       max_chunks_per_batch: int = 5) -> str:
    """
    T√≥m t·∫Øt t·ª´ file chunked_transcript.json v√† l∆∞u k·∫øt qu·∫£
    
    Args:
        input_file: File input ch·ª©a chunks
        output_file: File output ch·ª©a b√°o c√°o t√≥m t·∫Øt
        max_chunks_per_batch: S·ªë chunks t·ªëi ƒëa m·ªói batch
    
    Returns:
        Path ƒë·∫øn file output
    """
    print("üìÅ Loading chunks from file...")
    
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            chunks = json.load(f)
        
        print(f"‚úÖ Loaded {len(chunks)} chunks from {input_file}")
        
    except FileNotFoundError:
        print(f"‚ùå File {input_file} not found")
        return None
    except Exception as e:
        print(f"‚ùå Error loading file: {e}")
        return None
    
    # T√≥m t·∫Øt chunks
    import asyncio
    summary_result = asyncio.run(summarize_chunks(chunks, max_chunks_per_batch))
    
    # L∆∞u k·∫øt qu·∫£
    print(f"\nüíæ Saving summary to {output_file}...")
    
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary_result, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Summary saved to {output_file}")
        return output_file
        
    except Exception as e:
        print(f"‚ùå Error saving file: {e}")
        return None

