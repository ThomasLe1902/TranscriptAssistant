import os
from dotenv import load_dotenv
from typing import List, Dict, Any
import uuid

# Import Pinecone at module level
try:
    from pinecone import Pinecone
    import pinecone as pc_old
    PINECONE_AVAILABLE = True
except ImportError:
    PINECONE_AVAILABLE = False
    print("‚ö†Ô∏è Pinecone not available")

load_dotenv()
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_environment = os.getenv("PINECONE_ENVIRONMENT")
pinecone_index_name = os.getenv("PINECONE_INDEX_NAME", "transcript-assistant")

# Global model variable
_model = None
_use_mock = False

def get_pinecone_index():
    """
    L·∫•y Pinecone index v·ªõi c√°ch t∆∞∆°ng th√≠ch
    """
    if not PINECONE_AVAILABLE:
        raise Exception("Pinecone not available")
        
    try:
        if pinecone_environment:
            # C√°ch c≈©
            pc_old.init(api_key=pinecone_api_key, environment=pinecone_environment)
            return pc_old.Index(pinecone_index_name)
        else:
            # C√°ch m·ªõi
            pc = Pinecone(api_key=pinecone_api_key)
            return pc.Index(pinecone_index_name)
            
    except Exception as e:
        print(f"‚ùå Error getting Pinecone index: {e}")
        raise e

def get_model():
    """Lazy loading c·ªßa sentence transformer model"""
    global _model, _use_mock
    if _model is None and not _use_mock:
        try:
            from sentence_transformers import SentenceTransformer
            _model = SentenceTransformer('all-MiniLM-L6-v2')
            print("‚úÖ Using sentence-transformers model")
        except ImportError:
            print("‚ö†Ô∏è sentence-transformers not found, using mock embeddings")
            _use_mock = True
    return _model

def create_mock_embedding(text: str) -> List[float]:
    """T·∫°o mock embedding 384 dimensions (t∆∞∆°ng th√≠ch v·ªõi all-MiniLM-L6-v2)"""
    import random
    text_hash = hash(text) % (2**32)
    random.seed(text_hash)
    embedding = [random.random() for _ in range(384)]
    random.seed()
    return embedding

def create_embedding(text: str) -> List[float]:
    """
    T·∫°o embedding s·ª≠ d·ª•ng all-MiniLM-L6-v2 model ho·∫∑c mock
    """
    if _use_mock:
        return create_mock_embedding(text)
    
    model = get_model()
    if model is None:
        return create_mock_embedding(text)
    
    embedding = model.encode(text)
    return embedding.tolist()

def insert_data(texts: List[str], metadatas: List[Dict[str, Any]]):
    """
    Vector h√≥a v√† l∆∞u texts v√†o Pinecone
    """
    try:
        # Initialize Pinecone v·ªõi c√°ch t∆∞∆°ng th√≠ch
        index = get_pinecone_index()
        
        vectors_to_upsert = []
        for i, (text, metadata) in enumerate(zip(texts, metadatas)):
            embedding = create_embedding(text)
            vector_id = str(uuid.uuid4())
            vector = {
                "id": vector_id,
                "values": embedding,
                "metadata": {
                    **metadata,
                    "text": text  # Th√™m text v√†o metadata ƒë·ªÉ query
                }
            }
            vectors_to_upsert.append(vector)
        
        batch_size = 100
        for i in range(0, len(vectors_to_upsert), batch_size):
            batch = vectors_to_upsert[i:i + batch_size]
            index.upsert(vectors=batch)
        
        return index
        
    except Exception as e:
        if "No active indexes found" in str(e) or "not found" in str(e).lower():
            raise Exception("Pinecone index 'subtitles' not found. Please create it first.")
        else:
            raise e

def query_data(query: str, k: int = 5, video_id: str = None) -> List[Dict[str, Any]]:
    """
    T√¨m ki·∫øm semantic trong Pinecone
    
    Args:
        query: C√¢u h·ªèi t√¨m ki·∫øm
        k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
        video_id: ID video ƒë·ªÉ filter (optional)
    """
    try:
        # Initialize Pinecone v·ªõi c√°ch t∆∞∆°ng th√≠ch
        index = get_pinecone_index()
        
        # T·∫°o embedding cho query
        query_embedding = create_embedding(query)
        
        # Chu·∫©n b·ªã filter v·ªõi syntax ƒë√∫ng cho Pinecone
        filter_dict = None
        if video_id:
            filter_dict = {"video_id": {"$eq": video_id}}
            print(f"üîç Querying with video_id filter: {video_id}")
        else:
            print("üîç Querying without video_id filter")
        
        # T√¨m ki·∫øm
        results = index.query(
            vector=query_embedding,
            top_k=k,
            include_metadata=True,
            filter=filter_dict
        )
        
        # Format k·∫øt qu·∫£ v√† debug
        formatted_results = []
        for match in results.matches:
            match_video_id = match.metadata.get("video_id", "unknown")
            print(f"üìù Found match with video_id: {match_video_id}, score: {match.score:.3f}")
            formatted_results.append({
                "text": match.metadata.get("text", ""),
                "metadata": match.metadata,
                "similarity_score": float(match.score)
            })
        
        print(f"‚úÖ Total results found: {len(formatted_results)}")
        return formatted_results
        
    except Exception as e:
        print(f"‚ùå Error in query_data: {e}")
        return []